# 回顾

上节课我们讨论了深度学习的优化算法，包括SGD动量，也可以对SGD动量进行一点微调。也讨论了正则化，尤其要记住的是 dropout，它在正向传播过程中将网络的某些部分随机的设置为0，然后在测试时消除这些随机性。我们发现在不同的正则化中，这是普遍的模式：在深度学习的训练中加入一些噪声，在测试时再将噪声边缘化。我们还讨论了迁移学习，可以先下载一些相似的大的数据集预训练网络模型，然后根据自己的实际任务进行微调。这个方法可以帮助你在没有很大数据集的情况下也能完成深度学习任务。

# 前言

今天要讲一些关于编写软件和硬件，以及它们如何工作的细节。以及了解一下软件的实际应用流程，讨论一些主流的 CPU 和GPU。然后讨论几个主流的深度学习框架。

# CPU vs GPU

我们已经知道，计算机有CPU和GPU，深度学习使用GPU。但是并没有指出它们具体是什么，以及为什么在深度学习中GPU的表现要更好。

## CPU的介绍

CPU的全程是 Center Processing Unit ,中央处理器。就是下图这个小芯片。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/image-20220416230736091.png" alt="image-20220416230736091" style="zoom:50%;" />

## GPU的介绍

CPU 的全程是 Graphic Processing Unit，图形处理器。相对于CPU来说，GPU的体积大得多，它有自己的冷却系统，并且耗电更多。

从GPU的名字就可以看出来，他最初是被用于渲染计算机图形，是围绕游戏而开发的。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/image-20220416231035325.png" alt="image-20220416231035325" style="zoom: 50%;" />

我们知道的GPU一般有NVIDIA 和 AMD。深度学习中，我们基本上选择NVIDIA。使用AMD的显卡来做深度学习可能会遇到各种麻烦。

事实上，NVIDIA 一直大力推动深度学习。他们投入了很多人力物力来让他们的显卡更适合深度学习。所以，谈起深度学习中的GPU通常实质NVIDIA的GPU。

## CPU和GPU对比

CPU和GPU都是一种通用的计算机器。但是在性质上极为不同。

CPU的核数很少，可以利用多线程技术。也就是说，他们在硬件上可以同时运行多个线程。线程可以实现很多操作，并且运行速度非常快，运作相互独立。

+ GPU有上千上万的核数。GPU的每一个核都运行缓慢，能执行的操作没有CPU多。
+ GPU的核不能独立运行，它们需要共同协作。因为GPU有大量的核，当你需要同时执行操作时，它的并行能力很强。

CPU 跟 GPU 还有一点需要指明的是内存的概念。

+ CPU 有高速缓存但是相对比较小，而且CPU 的大部分内存都是依赖于系统内存。在一台典型的消费级台式机上- RAM -的容量可能有8，16或者32GB。
+ GPU 中内置了 RAM。 GPU 与系统RAM通信时，会带来产重的性能瓶颈，因此GPU 基本上拥有自己相对较大的内存。Titan XP 它的本地内存有12个GB。GPU 也有它自己的缓存系统，所以在 GPU的12个 G和GPU 核之间有多级缓存。它跟 CPU 的多层缓存类似。

![image-20220416232211582](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/image-20220416232211582.png)

## GPU 的并行能力

CPU 对于通信处理来说是友好的。而 GPU 更擅长于处理高度并行的算法其中最典型的的就是就是处理矩阵乘法。

下图中，左图是一个 A*B 的矩阵，乘以一个 B * C 的矩阵，最后得到一个规模为 A*C的输出矩阵。对于输出矩阵来说，每一个元素都是第一个矩阵的某一行与第二个矩阵的某一列的点积的结果。这些点积运算都是相对独立的。想象一下，将输出矩阵中每个元素并行计算，并且所有的计算都是两个向量点积的运算。实际上就是从两个输入矩阵的不同位置进行读取数据，然后进行点积，填到对应位置上。这是一个典型的适用于CPU解决的问题。

如果使用CPU来进行矩阵乘法的运算，可能会进行串行计算，就是对每个元素一个个进行计算。现在，CPU拥有多核，也可是进行矢量运算，但是针对并行任务，CPU通常表现得更好。特别是当矩阵规模非常庞大的时候。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/image-20220418000318804.png" alt="image-20220418000318804"  />

## GPU 计算架构

可以在 GPU 上写出可以直接运行的程序。NVIDIA 有个叫做 CUDA 的抽象代码，可以让你写出类 C的代码，并且可以在 GPU上直接执行。但是想要写出高性能
并且充分发挥 GPU 性能的 CUDA 代码，是很困难的。你必须非常谨慎地管理内存结构，并且不遗漏任何一个高速缓存，以及分支误预测等等。所以自己编写高性能的 CUDA 代码是非常困难的。

NVIDIA 开源了很多的库，这些库实现了通用的计算语言，可以用来实现高度优化。举个例子，NVIDIA 有个叫做 **cuBLAS** 的库，可以**实现各种各样的矩阵乘法**，并且矩阵操作都是被高度优化的，可以在 GPU 上很好地运行，非常接近硬件利用率的理论峰值。还有一个叫做**cuDNN** 的库可以**实现卷积，前向和反向传播，批量归一化，循环神经网络等**。还有另一种语叫做**openCL**，这种语言在深度学习中更加普及**。它不仅可以在 NVIDIA GPU 上运行，还可以在 AMD 以及 CPU 上运行。**但是没有人花费大量的精力优化 openCL，所以 openCL 在性能上并没有 CUDA 好。

## CPU和GPU性能表现

将Intel 的 CPU8-14 和当时性能最好的GPU Pascal Titan X做一下性能比较，下图是测试的结果。更详细的结果可以 [参考这里。](https://github.com/jcjohnson/cnn-benchmarks)对于VGG 16/19 和不同层数的ResNet ，**对于同样的计算任务，CPU耗时是 GPU的65到75倍。**这个测试对CPU有些不公平，因为没有压榨出CPU的最大性能。只是在CPU直接安装运行了torch。



![image-20220418004740182](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/image-20220418004740182.png)

另一个有趣的结果是,比较了卷积优化的 cuDNN 库和没有经过优化的 直接以 CUDA 写成的代码，在相同的网络相同的硬件相同的 Deep Learning 框架上，CUDA 版代码可以在图表中看到大约有3 倍的速度差距。也就是说优化过的 cuDNN比原生 CUDA版代码快这么多。所以一般来说只要你在 GPU 上写代码,你就应该使用 cuDNN.

![image-20220418163640703](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/image-20220418163640703.png)

## CPU和GPU的通信

在训练网络的时候，你的模型可能存储在 GPU 上，但是庞大的数据集却存储在机械硬盘或者是固态硬盘上。从硬盘中读取数据的很可能成为训练速度的瓶颈。因为 GPU 非常快，它计算正向反向传播的速度非常快，从硬盘上串行地读取数据会拖慢训练速度，这会让训练变慢。

有一些解决的方法：如果数据集比较小，你可以把整个数据集读到内存中。或者数据集不小但是你有台内存足够大的服务器，也可以这么干。或者装固态硬盘替换掉机械硬盘，提升读取数据速度。

另一种常用的思路是：**在 CPU 上使用多线程来预读数据**。把数据读出内存或者读出硬盘存储到内存上，这样就能连续不断地将缓存数据高效地送给 GPU。这种方法不太容易实现，因为 GPU 太快了，如果不能及时把数据发送给 GPU，仅仅读取数据这一项就会拖慢整个训练过程。