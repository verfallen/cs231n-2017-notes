# 回顾

上节课我们讨论了深度学习的优化算法，包括SGD动量，也可以对SGD动量进行一点微调。也讨论了正则化，尤其要记住的是 dropout，它在正向传播过程中将网络的某些部分随机的设置为0，然后在测试时消除这些随机性。我们发现在不同的正则化中，这是普遍的模式：在深度学习的训练中加入一些噪声，在测试时再将噪声边缘化。我们还讨论了迁移学习，可以先下载一些相似的大的数据集预训练网络模型，然后根据自己的实际任务进行微调。这个方法可以帮助你在没有很大数据集的情况下也能完成深度学习任务。