# 回顾

上节课我们讨论了不同的深度学习框架,讨论了 PyTorch，Tensor Flow还有 Caffe2。深度学习框架的优点有：

+ 快速构建大型的运算网络，例如大规模神经网络和卷积神经网络。
+ 快速计算网络中的梯度，同时能够计算所有中间变量的权重并用来训练模型。
+ 在 GPU 上高效运行

这些框架主要是通过调制神经网络中的前向层和后向层来工作。使用时只需要定义神经网络层的顺序。就可以很快构建一个很复杂的神经网络架构。

# 介绍

会讨论一些特定类型的卷积神经网络架构，在研究和实际应用中使用得很广泛。深入探讨那些 ImageNet大赛获胜者用的最多的神经网络架构，按照时间顺序它们分别是是 AlexNet，VGGNet，GoogleNet 和 ResNet。然后简单介绍些其他的目前并不常用的架构。

# LeNet

LeNet可以看作是卷积网络的第一个实例，并且在实际应用中取得成功。

下图就是 LeNet 的结构，输入一个图片，使用步长为1大小为5x5的卷积核，接下来重复卷积和池化操作，最后有一些全连接层。LeNet在数字识别领域的应用方面取得了成功

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204221352033.png" alt="image-20220422135217947" style="zoom:67%;" />

# AlexNet

AlexNet是第一个在 ImageNet 的分类比赛中获得成功的大型卷积神经网络。AlexNet 在2012年参赛，之前的非深度学习架构相比，它大幅提高了识别准确率。从此开始了大规模对卷积神经网络的研究和应用。

下图是**AlexNet 的基础架构**：卷积层，池化层，归一化层，卷积，池化，归一化层然后是三个卷积层一个池化层，最后是三个全连接层。它看上去和 LeNet 很相似，区别在于总层数变多了，卷积层达到了五层，在最后的全连接层输出分类之前，多了两层全连接层。

![image-20220422144714052](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204221447154.png)

## 详解AlexNet 架构

### 第一层卷积

第一个卷积层有96个步长为4的大小为11×11的卷积核

输入：AIexNet 的输入也就是 ImageNet 上的数据集，227x227x3 的图像矩阵。

输出：卷积层输出的维度是 55x55x96。，卷积后的长，宽可以根据公式$(width +2 * padding - filter)/stride + 1$ 算出，在这个例子中，就是 $ (227-11)/4+1=55$。图像的长宽相等，所以卷积后的长款也是相等的，都是 55 。卷积后的深度就是卷积核的个数，即 96。

参数数目： 96\*11\*11\*3，我们有96个 11 * 11 的卷积核，每一个卷积核都会处理一个11 * 11 * 3的数据块。因为输入数据的深度是3，所以卷积核的深度是3。第一层参数总共是 96 * 11 * 11 * 3 个。

### 第二层

第二层是池化层，采用步长为2，进行池化操作。

输出维度：27 * 27 * 96，因为池化层会保留数据深度，所以我们输入的数据深度是96，那么输出的深度也是96。

参数数目：0，池化层是不需要参数的。参数是我们需要训练的权重，在池化的操作中，我们只是观察池化区域并取最大值，没有需要训练的参数。

### 全连接层

依词类推，我们得到AlexNet 架构，包括完整的输入和输出的维度。

1. [227x227x3] 输入
2. [55x55x96] 第1个卷积层: 96个11x11的卷积核，步长为4，填充为0 
3. [27x27x96] 第1个最大池化层：3x3 卷积核，步长为 2
4. [27x27x96] 第1个归一化层: 归一化层
5. [27x27x256]  第2个卷积层: 256个5x5 卷积核，步长为1，填充为2
6. [13x13x256] 第2个最大池化层：3x3 卷积核，步长为 2
7. [13x13x256] 第2个归一化层: 归一化层
8. [13x13x384] 第3个卷积层: 384 个3x3 卷积核，步长为1，填充为1
9. [13x13x384] 第4个卷积层: 384 个3x3 卷积核，步长为1，填充为1 
10. [13x13x256]  第5个卷积层: 256个3x3 卷积核，步长为1，填充为1 
11. [6x6x256] 第3个最大池化层：3x3 卷积核，步长为 2
12. [4096] FC6: 4096 个神经元
13. [4096] FC7: 4096 个神经元
14. [1000] FC8: 1000 个神经元 (class scores)

最后的层是 FC8，它连接一个soft max 函数，进行1000 个类别的 Image Net 图像分类。

### 一些训练细节

1. 这是**第一次使用 ReLu 非线性函数**
2. 使用了本地归一化，通过相邻通道来归一化响应。现在已经被摒弃了，有研究表明它不太有效果。
3. 在提出AlexNet 中的论文里，**有大量的数据增强操作**，如翻转、晃动、裁剪、颜色归一化等。当并展类似例子中的工程时，这些方法非常有用。
4. 使用了Dropout 
5. batch size 设置为128
6. 使用了带有动量的SGD，系数为0.9
7. 一般由不大的学习率，从$10^{-2}$开始学习，每到鞍点时，学习率除以10，直到网络收敛，训练结束。
8. 使用了权重衰减
9. 集成训练多个模型，然后取平均来提高性能（错误率从18.2% 降到了15.4%）
10. **将网络拆成两部分，在两个GPU里运行**。（对应架构图中第一个卷积层开始分为了两部分）这主要是因为历史原因，Alexnet曾经的训练平台是 GTX580 GPUS，这是一种老的GPU，只有3G的内存。它无法容纳整个神经网络，最后的解决方案是将网络拆分成两部分，在两个GPU里运行，每个 GPU 只容纳半数的神经元。以第一个卷积层为例，这里55x55x96 的数据输出，仔细观察这个网络的图解，每个 GPU 的实际深度只有48。在 6，7，8 层，也就是 FC层中，
    GPUS 互相通信。

# ZFNet

AlexNet是2012年 ImageNet 分类问题的冠军，比起2011年，错误率上有很大降低。这是第一次基于 CNN 的冠军，后来，它被转换成不同的版本适用不同的任务
，应用了很长一段时期。

![image-20220423195655451](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/image-20220423195655451.png)

2013 年的 ImageNet 比赛中，优胜者是 **ZFNet**，它与用于与AlexNet 相同的层数，相同的基本结构，只是基于AlexNet 上进行了改动。

下面是ZFNet 的结构图，它改动的地方在于

1. 改变了第一个卷积层卷积核的大小及步长，AlexNet 使用 11\*11，步长为4的卷积核，ZFNet 使用 7\*7,步长为2的卷积核。
2. 改变第3,4,5个卷积层的卷积核数目，分别使用 512,1024,512 代替原本AlextNet 的384,384,256 。

![image-20220423200140078](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/image-20220423200140078.png)

## 

# VGGNet

2014年比赛出现了一些改进的架构，它们和之前网络**最大的不同是拥有了更深层的网络**。从2012年2013年的8层网络，到2014 年，两个非常接近的获胜者别是19层，22层。分别是 来自 Google 的优胜者 GoogleNet和来自 Oxford 的 VGGNet。这些都是非常鲁棒的网络。

**VGG 的思想是一个包含小卷积核的非常深的网络。**从 AlexNet的8层扩展到到VGGNet 的16-19层。关键点是他们保持小的卷积核，只用3*3的卷积（基本上是最小的卷积大小，只关注相邻的像素）定期下采样，这种思想贯穿整个网络，它是非常简炼优雅的网络，获得了ImageNet最高的7.3的错误率。

![image-20220423203103043](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/image-20220423203103043.png)

**<u>Q：为什么使用最小的卷积核？</u>**

A：当使用小的卷积核，参数量比较小。**使用小的卷积核让我们可以尝试更深层的网络和更多的卷积核。**使用多层小的卷积核，比起使用大的卷积核，感受野范围更大。比如堆叠3层 3\*3 步长为1的卷积核，它和一个7\*7 的卷积核拥有相同的感受野范围，但是网络更深，层数更多，有更多的非线性操作，整体上来说，参数更少。使用3层 3x3的卷积核，C为通道数（比如RGB通道，则C=3），总参数个数为 3x3x3xCxC 个。使用 1层7\*7 的卷积核，总参数个数为 7\*7*\*C\*C。

**<u>Q：是什么叫更深，是指卷积核数量还是层数？</u>**
A：通常例子中的更深是指层数，深度有两个用法，一个是通道的深度，也就是宽乘以高乘以深度。另一个是指网络中网络层的数量，指可训练权重层的总数量。

## 感受野

> 在卷积神经网络中，感受野的定义是 卷积神经网络每一层输出的特征图（feature map）上的像素点在**原始图像**上映射的区域大小。 ——博客园

关于感受野的详细内容，可以[点击这里](https://zhuanlan.zhihu.com/p/31004121)

