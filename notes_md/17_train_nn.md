# 迁移学习

过拟合是因为数据不够。你希望能得到一个功能强大的网络，在使用小数据集合时就很容易过拟合。正则化是处理过拟合的一种方法，另一种方法是**迁移学习**。

> 迁移学习是一种机器学习方法，就是把为任务 A 开发的模型作为初始点，重新使用在为任务 B 开发模型的过程中。

迁移学习能够打破神经网络必须要用大数据集这种桎梏。它的思想很简单：

1. 首先找到一些卷积神经网络，比如下图中最左边是 VGG 架构的网络，用足够的其他的数据去训练整个网络。例如使用 ImageNet 。

2. 把这个数据集中训练出来的能提取特征的能力运用到更小的你感兴趣的数据集上

   这个小的数据集不需要像ImageNet 那样分1000类，假设它有C类。一般做法是修改最后一层——特征到最后分类输出的全连接层，重新初始化这部分矩阵。对于ImageNet来说，它是 4096*1000维的矩阵。对于新的分类矩阵的大小，变成了 4096 * C 维。冻结前面层的权重，只需要重新训练这个线性分类器，让它在你的数据上收敛即可。

3. 如果你有更多的数据，你可以更新网络的更大一部分。一个通用的策略是将学习率调低（原始的学习率的1/10）。因为最初的网络参数可能是在ImageNet 上已经收敛的，泛化能力已经很强了。你只希望让它们有微小的调整来适应你的数据集。

![image-20220414202816015](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204142028099.png)

使用迁移学习时，如果你的数据和一些大数据集的图片很相似，但是你的数据量很少。例如ImageNet 有很多动植物之类的图片，你只是想给动物植物或者其他类别的图片分类。那就很好办了。**你可以在ImageNet 上预训练模型，然后在此基础上只训练最后一层线型分类器。**如有更多的数据可以微调一下模型。

如果你的数据没有与其相似的大数据集，比如你要处理X光或者CT图像，可能需要加上一点创造力。你可以考虑重新初始化大部分的网络，多做一些实验。在数据量比较大的情况下，这种方法不错，因为你可以精调大部分网络。

![image-20220414204553228](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204142045296.png)

## 迁移学习思想的普遍性

转移学习的思想是超级普遍的。如果阅读读机器视觉的论文，经常能看类似这样的系统图。
针对不同的任务（下图左边是目标检测，右边是图像加标），所有的模型都有一个卷积神经网络处理图像的模块。目前无论是计算机视觉的哪方面应用，大多数人都不会从头训练这些东西。
**大多数情况下，在 Image Net 上预训练然后根据任务精调。**同样的，在图像加标的环境下有时候可以预先训练一些和一些语言相关的词向量。你可以在 Image Net 上预训练卷积神经网络，在一些文本字典上预训练一些词向量。然后针对你的网络精调，不过在加标任务中预训练词向量的方法不太常见。
要记住，对于你要处理的问题，没有大数据集，你应该做的事是下载一些相关的预训练的模型，然后，重新初始化部分模型或者在你的数据上精调模型。这是一个很普遍的策略。

![image-20220414205944995](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204142059108.png)

在所有不同的深度学习软件包上都提供模型库，在这里可以下载不同模型的预训练版本。

+ Caffe: https://github.com/BVLC/caffe/wiki/Model-Zoo
+ TensorFlow: https://github.com/tensorflow/models
+ PyTorch: https://github.com/pytorch/vision 

##  小结

- 最优化是用来改进训练效果的，
- 正则化是用来改变在测试集上的性能，集成模型也算是其中一种。
- 迁移学习可以让你在小样本的时候训练的一样好。
  