# 15 训练神经网络(下)

# 回顾

快速回顾下我们上节课的内容。

## 激活函数

我们讨论了各种各样的激活函数。我们看到，大概在10年前，sigmoid 激活函数曾经在训练神经网络方面十分流行，但是存在梯度消失的问题。tanh 函数也存在类似的问题。对于这样的问题一般的建议是你可能想要在大多数情况下，把Relu 函数作为默认的激活函数。因为它在不同的框架下都能够运行的很好。

![image-20220412184855745](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121848814.png)

## 权重初始化

我们也讨论了权重初始化，首先要记住的是，当你们在开始训练的时候，初始化你们的权重值（即参数w）,如果那些权重的初始值太小，你就会发现激活值消失。因为你不断乘以这些很小的数，那么最终他们就会衰减为0，学习也就无从谈起。

从另一个角度来说 ，如果你的权重初始值太大，那么这些初始值不断地乘以你的权值矩阵，最终会产生梯度爆炸，无法学习。

但是如果你正确地初始化参数，举个例子，使用Xavier 初始化法 或者MSRack初始化法，那么在你学习神经网络的每一层，激活值都有很好的分布。记住在深度网络越来越深的情况下，权重初始化会变得至关重要。因为随着网路的变深，将不断地乘以那些权值矩阵。

## 数据预处理

我们讲过，在卷积神经网络中，中心化和归一化是非常常用的手段。它会使数据分布均值为1，方差为0。

![image-20220412184933962](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121849059.png)

我想在直观的讲一下这样做的原因：举一个简单的例子，我们要通过二元分类的方法分离这些红色和蓝色的点，从左边的图来看，如果数据点没有被归一化和中心化，而且距离坐标原点很远，虽然我们仍然可以用一条直线分离它们，但是如果这条直线稍微转动一下，那么我们的分类器将被完全的破坏。这意味着，在左边的例子中，loss 对我们的权重矩阵中的线性分类器的小波动非常敏感。我们仍然可以表示相同的函数，但是这会让深度学习异常艰难。因为它们的损失对我们的参数向量非常敏感。

而在右边的情况下，如果你使用数据集的时候，将数据点移动到原点附近，并且缩小它们的单位方差，我们仍然可以很好地对这些数据进行分类。但当我们稍微转动直线时，损失函数对参数值中的小波动就不那么敏感了。这可能使得优化变得更容易一些的同时，能够看到一些进步。而且，这种情况不仅仅在线性分类中遇到。记住，在神经网络中我们需要交叉地使用这些线性矩阵相乘或者卷积，还有非线性激活函数。如果神经网络中某一层的输入均值不为0或者方差不为1，该层网络权值矩阵的小波动就会造成输出的巨大波动，从而造成学习困难。所以，这就直观的解释了为什么要归一化。

![image-20220412184948257](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121849323.png)

还要记住，因为我们了解归一化的重要性，所以引入了Batch normalization 的概念。即在神经网络中加入额外一层，以使得中间的激活值均值为0，方差为1。在这里，我通过更直观的形式总结了Batch Normalization 的方程。在batch normalization 中，正向传播的过程，我们使用小批量数据计算平均值和标准差，并使用这个估计值来对数据进行归一化。之后我们还介绍了缩放参数和平移参数增加层的可表达性。

![image-20220412185009320](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121850405.png)

## 跟踪学习过程

上次我们还介绍一部分跟踪学习过程，比如在训练过程中，如何观察损失曲线。以下是一些神经网络的例子，这是我在周末实际训练过过程中发现的。这通常是我在研究问题中的一些例行公事。

![image-20220412185054027](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121850126.png)

左侧是随时间变化的损失函数曲线，你可以看到曲线多多少少是下降的。说明神经网络的损失在下降，这是一个好的信号。右侧曲线，x轴同样是训练时间或者迭代次数，y轴表示模型在测试集和验证集上的效果。你会发现，随着时间增加，训练集上的效果随着损失的下降到某一点之后不再上升。这说明模型进入了过拟合状态，这时候就需要加入其他的正则化手段。

## 超参数搜索

所有这些神经网络都涉及到大量的超参数，找到正确的参数十分重要。我们讲到了**网格搜索**，以及**随机搜索在理论上的优越性**在哪里。因为当你的模型性能对一个参数比对其他超参数更敏感时，随机搜索可以对超参数空间覆盖地更好。我们还介绍了==**粗细粒度交叉搜索==**，当你做超参数优化时，一开始可以会处理很大的搜索范围，几次迭代之后，就可以缩小范围，圈定合适的超参数范围。然后再对这个小范围，重复上述步骤，以获得超参数正确的区域。另外很重要的一点是，一开始你要确定超参数粗略的范围，这个范围要非常宽，覆盖你所有的超参数。理想情况下，范围应该足够宽到你的网络不会超过范围的任意一边。这样你就知道自己的范围足够大。

![image-20220412185104054](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121851181.png)

Q：通常一次搜索几个超参数

A：例子中给的是2个，通常会超过2个，这主要取决于你的模型和架构。由于超参数可能性的数量是以指数增长的，所以其实一次性没办法处理太多的超参数。这还取决于有多少设备能为你所用，所以这是因人而异，因实验而异的。通常我每次不会处理超过2个，3个甚至4个超参数。因为这会让指数搜索失控。通常  **学习速率**  是最重要的，首先确定它，其他的诸如正则化，学习速率衰减稀疏，模型大小等参数不那么敏感，跟学习速率相比。所以一般需要反复迭代，找到最佳学习速率然后返回，去寻找不同的模型大小。这能帮你减少实验搜索的时间。这个问题通常取决于以 什么样的顺序搜索数据。

Q：当你改变一个超参数时，其他超参数的最佳值有多大可能性会改变？

A：这有时确实会发生，虽然对于learning rate来说这方面一般不是问题。对于learning rate来说，你需要找到一个好的范围，然后设定得比最佳值稍微小一点，让他运行很长时间。采取这种方法，再加上优化策略——我们今天要学的，你会发现很多学习模型在你找到好的范围之后对learning rate 就不那么敏感了。

Q：降低learning rate，增加时间点的数量有什么问题吗？

A：你可能就就会花费更多的时间 😄。 确实，直观上看，如果你把learning rate 设的很低，让他持续很长时间，这在理论上是行的。但实际上，参数是10还是100对实验室很重要的。如果采用何时的learning rate ，可能几个小时或者一天完成训练。如果你只是为了保险，把10换成了100，那么1天的训练可能就变成了100天的训练。这就像是你去上计算机课，通常会忽略常量，但是在实验中，常量就非常重要了。

Q：对于learning rate 会不会出现卡在局部最小值的情况？

A：直觉上是可能的，但实际上并不是。我们今天会讨论这个问题。

今天我要说的是其他几个有趣又重要的话题，它们都跟神经网络训练有关。特别要说的是，我们提过更有效的优化方法。今天我想花点时间深入介绍。以及近年来人们用的最多的优化算法。我们也会涉及之前提到的正则化 ，这可以减少训练和测试误差之间的鸿沟。 

谈到神经网络，我更多想讲的是人们实际使用的正则化策略。最后我想讲一下**迁移学习。**当你拥有的数据比想象的时候少时，你可以通过它将一个问题转换为另一个问题。

# 优化

回想一下之前的课程，训练神经网络的核心策略是一个优化问题。我们写下损失函数，定义网络权重的每一个值，损失函数告诉我们这些权重值在解决我们的问题时表现是好是坏。我们设想在当前权重下，损失函数给了我们漂亮的等高线图。这是一个二维问题，X 和Y轴表示两个权重值，图上的颜色表示损失值。在这个二维问题的卡通图中，我们只优化两个值 $w_1$ 和$w_2$，目标是找到红色最深的区域。在这种情况下，对应了损失最小的权重值。

记住我们已经使用过最简单的优化算法-SGD，它非常简单，只有三行代码。我们首先评估一下小批量数据中损失的梯度，然后向梯度的负方向更新参数向量。因为它给出了损失函数下降最快的方向，然后重复这个过程。幸运的化，它在红色区域收敛，我们如愿得到很小的误差值。遗憾的是，这个相对简单的优化算法在实际使用中会产生很多的问题。

![image-20220412185115633](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121851753.png)

## SGD的问题

SGD 的问题之一 ：想象一下我们的目标函数发生了什么，就像这样，我们画两个值，W_1 和W_2 ，当我们在水平方向上改变值，损失函数变化非常慢、当我们在等高线上下方向运动时 ，损失值则对垂直方向的变化非常敏感。对于损失值来说，在这一点上是很坏的情况。在这一点，它是海森矩阵中最大奇异值与最小奇异值之比。但是直观来看，损失值等高线图就像是一个玉米卷饼，它在一个方向上非常敏感，而在其他方向上不敏感。问题是对于一个像这样的函数，SGD会做什么？如果你在这类函数行运行SGD，就会的得到下图这样的之字图形。这是因为这类目标函数梯度的方向并不是与最小值成一条线，当你计算梯度并沿着前进时，你可能一遍遍跨过等高线，之字形地前进或后退，所以你在水平方向上前进速度非常慢，在这个方向上敏感度较低。但是在非常敏感的垂直维度上， 对水平方向梯度不敏感，这并不是我们所希望的，而且，事实上，这个问题在高维空间变得更加普遍。在这里，我们只展示了两维优化等高线图，在神经网络中，可能存在上百万甚至上亿个参数。它会沿着上亿个方向进行移动，在不同的运动方向上，介于最大值和最小值的方向上的比例很高，SGD的表现并不好。你可以想象一下，我们有上亿个参数，在它们两者之间的最大比例可能很大，因此，我认为在多维问题中，这是一个大问题。

![image-20220412185125097](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121851197.png)

SGD的另一个问题是局部最小值或鞍点。这里我把图形做小小改动，X轴显示参数的值，Y轴显示损失值。在上面的例子中，我们有这类函数的目标函数，在曲线中间有一段凹陷。这种情况下，SGD会卡在中间。因为那里是局部最小值，梯度为0，因为那一段是平的。还记得SGD计算梯度，向着梯度相反的方向前进，在目前的点上，相反的梯度值为0，我们会被卡在这个为止。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121851945.png" alt="image-20220412185140914" style="zoom: 33%;" />

关于鞍点，还有另一个问题，相比于局部最小值，你可以设想在一点上，往一个方向是是向上，另一个方向是向下，在这种情况下，损失函数会被卡在鞍点。因为在这里梯度为0。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121851752.png" alt="image-20220412185157721" style="zoom: 33%;" />

我希望指出一点，在一维问题上，局部极小值看起来是个大问题，鞍点看起来并不需要担心。但是事实上，一旦涉及到高维问题恰好相反。想象一下一亿参数的空间，鞍点意味着在当前点上，某些方向的损失会增大，某些方向的损失会减小。如果维度是一亿，它会发生的更加频繁，几乎任何点上都会发生。然而在局部极小值点上，在一亿个方向上，任何一个方向前进损失都会变大，事实上，当你考虑这种很高维问题时，这种情况很稀少。这个问题时最近这几年在训练非常大的神经网路时才显示出来。问题更多的出现在鞍点上，局部最小值问题少一点。不过，有时问题并非恰好在鞍点上，也可能在鞍点附近。如果你看看鞍点附近，就可以看见鞍点附近梯度并不是0，但是斜率非常小。这意味着如果我们向梯度方向前进，而梯度非常小，任何时候当当前参数值在目标等高线图上 接近鞍点时，进展会非常缓慢。

SGD的另一个问题是随机性。SGD是随机梯度下降，回忆一下，损失函数是通过多次重复计算不同实例的损失来定一个的。在这个例子中，如果N是这个训练集的长度，可能有100万个，每次计算损失都会耗费很大的计算量。事实上，我们通常通过小批量的实例来对损失和梯度进行估计。这意味着并不会每一步都去计算真实的梯度，而是在当前位置对梯度进行噪声估计。

下图是我对图做了一点修改，我只是在每一点的梯度上加入了随机均匀噪声，搞乱梯度，再这样的噪声条件下运行SGD，这可能并不完全是SGD发生的事情，但是这仍然能给你一种感觉，如果在你的梯度估计中存在噪声，那么常规的SGD这种周围曲折的空间可能实际上需要花费很长时间，才能得到极小值。

加入均匀噪声后，训练轨迹如下：

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121852815.png" alt="image-20220412185208720" style="zoom: 33%;" />

Q：如果我们使用正常的梯度下降，这些问题都会消失吗？

A：如果在全量梯度下降中加入噪声，噪声，像我们看到的一样，我们有时可能会在网络中引入额外的噪声，不仅仅是因为小批量采样，还因为网络中具有明确的随机性。这仍然会是一个问题。鞍点，对于全量梯度下降仍然是一个问题。因为在目标函数的全等高轮廓线中也会存在鞍点。

# SGD+ Momentum

为了解决上述的问题，我们在SGD中加入一个动量项。 下图左侧是经典的SGD，只在梯度方向上前进。但在右侧，有一个非常小的方差，称之为带动量的SGD。思想是，保持一个不随时间变化的速度，并且把梯度估计加到这个速度上。然后再速度的方向上前进，而不是在梯度的方向上前进。这里出现了一个代表摩擦系数的超参数 $\rho$ 。之后的每一步，我们采用当前的速度，然后用摩擦系数$\rho$  来对其进行衰减，再加到梯度上。($\rho$经常取值较大，通常选择0.9）这个简单的策略可以解决上述所说的局部最小值和鞍点问题。

![image-20220412194739582](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121947645.png)

那么在局部最小值点或者鞍点发生了什么，可以想象这个系统中的速度就像是一个球滚下山，他会在下降时加上速度。一旦加上速度，在通过局部最小点时虽然没有梯度，仍然还有速度，就能越过当前局部最小点。在鞍点处也是如此，虽然鞍点附近的梯度非常小，但我们还有下山时就建立起来的速度向量。这能够帮助球通过鞍点，并且继续滚动下去。

让我们思考一下，如果在曲折的梯度附近会发生什么? 一旦使用动量，这些之字形的曲折就会互相抵消，这能够有效减少我们朝敏感方向前进步数的数量。而在水平方向上，速度会不端增加，在其他不敏感的维度上，速度会加速下降。因此添加动量，实际上能够帮助处理高条件数的问题。下图中，下图重现了带有噪声的梯度下降过程，黑色的曲线代表常规的SGD，蓝色的曲线代表带有动量的SGD。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121940880.png" alt="image-20220412194030784" style="zoom:33%;" />

Q： SGD动量如何处理条件很差的坐标

A：回顾下速度估计和速度计算，就会发现我们在每一步都增加了梯度。这在一定程度上取决于你的摩擦系数$\rho$的设置。如果梯度较小，并且这种情况下 $ρ$ 表现地很好，我们的速度可以单调递增到一个速度比实际梯度更大的点，然后我们可能会更快地处理条件差的维度。

当我们在使用带有动量的SGD时，可以这样想象，红色的点是我们的当前为止，红色的向量表示梯度的方向，绿色向量是速度向量的方向，当我们做动量更新时，实际上我们是根据两者的平均权重进行步行。这有助于克服梯度估计中的一些噪声。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204121951864.png" alt="image-20220412195158832" style="zoom: 50%;" />

## Nesterov 动量

 当看到一个梯度的微量变化时，叫做 ==**Nesterov 加速梯度**==，也可以称为为 Nesterov 动量。他把这个顺序改变了一下，在普通的SGD动量中，我们估计梯度，然后取速度和梯度的混合。但是在 Nesterov 动量梯度中，你从红色的点开始，然后再取得的速度方向上进行步进，之后评估这个位置的梯度，随后回到初始位置，将这两者混合起来。