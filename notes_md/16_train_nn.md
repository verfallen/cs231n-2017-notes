# 正则化

如何提高单一模型的效果呢？**使用正则化**。正则化的思想是**在模型中加入一些成分来防止训练集上的过拟合，从而使测试集上的效果得到提升**。在前面已经经过几种正则化的方法，如下图所示，我们在损失函数上加入额外的一项（红框圈住的部分）。前面的一项是用来拟合数据，后面的一项是用来防止模型过拟合的，称为**正则项**。

![image-20220414134530394](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141345442.png)

常用的正则化方法如下：

![image-20220414134745917](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141347971.png)

## Dropout 

在神经网络中，L2 正则化在神经网络中效果不明确，因此我们选择其他方案。一个在神经网络中常用的方法就是 **Dropout**。Dropout 的思想非常简单，**每次在网络中正向传递时，我们在每一层网网络中，随机地将一部分神经元置为 0** 。每次进行正向传递时，随机被置零的神经元都不是完全相同的，在处理单层网络事，算出这一层的值然后随机将其中一些置为0，接着继续在网络中前进。

对比下面的左右图，左图是全连接网络，右图是经过Dropout 的版本，可以发现，**经过 dropout 之后的网络就是同样的网络变小了一号，只用到其中一部分神经元**。

![image-20220414135413015](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141354068.png)

<u>**Q：我们把什么置为0？**</u>

A：激活函数的输出值。每一层都是在计算上一层的激活函数的结果乘以权值矩阵，得到下一个激活函数前的结果。然后我们计算激活函数，将其中一部分置为0，那么下一层拿到的激活函数的输出里面就有一部分是0。

<u>**Q：在哪些层里使用Dropout？**</u>

A：一般在全连接层或者卷积层。但是在卷积层中使用 dropout，有时候并不是随机将某个神经元上的激活函数的输出置为0，而是随机把某个特征映射置为0。在CNN中，有一个维度表示通道，有可能要把某几条通道整个置为0,而不是将元素置0。

### 代码实现

dropout 的代码实现非常简单，只需要两行。下图是一个三层的神经网络中实现 dropout 的实例。

![image-20220414140256626](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141402706.png)

### dropout 为什么有用

一个比较勉强的解释是，人们认为dropout 避免了特征间的相互适应。举例来说要在分类中判断是不是猫，可能在网络中，一个神经元学到了耳朵，一个神经元学到了尾巴，另一个学到了输入图像有毛，然后将这些特征组合起来判断。加入dropout  后，网络不是依赖将这些特征组合在一起给出结果，而是通过不同的零散特征来判断。这也许在某种程度上抑制了过拟合。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141412424.png" alt="image-20220414141212377" style="zoom:50%;" />

另一个解释是，dropout 是在单一模型中进行的集成学习。观察经过dropout的网络，我们是在一个全连接网络的子网络中进行，也就是用全部神经元的子集进行运算。每一种可能的dropout 方式都可以产生一个不同的子网络。所以dropout 像是同时对一群共享参数的网络来进行集成学习。又因为 dropout 的可能性是随着神经元个数呈指数级增长的，你不可能穷举每种情况。这可以看做是一个超级无极大的网络集合在同时被训练。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141419485.png" alt="image-20220414141937411" style="zoom:50%;" />

### Test Time

当使用了dropout ,我们把神经网络基本的运算都改变了，引入了一个额外$z$，代表在dropout中被随机置零的项。但是在测试时引入随机性可能不是一个好主意，想象一下你在Facebook工作，想要对用户上传的图片进行分类，今天分类器的判断是猫，明天也不是了，这就很糟糕。我们希望一个网络一旦被训练好了，在测试时能够消除这种随机性，我们就会想要将随机性进行平均。如果把它写下来，可以想象通过积分来边缘化随机性。

![image-20220414153358697](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141533738.png)

但在实际中，难以对积分求解。可以通过采样来逼近这积分。对z进行对此采样，然后再测试时将采样结果平均化。这仍然会引入一些随机性。

在dropout 的情况下，可以使用更省事的方式来局部逼近这个积分。假设有这样一个神经元，$a=w_1 \cdot x + w_2 \cdot y$

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141537919.png" alt="image-20220414153710879" style="zoom:33%;" />

在训练时使用了dropout，丢弃神经元的概率是0.5。有4种可能的dropout掩码集合，通过将这4个掩码的集合进行平均，可以求出训练期间的期望值。可以看出，训练时的平均值只有测试的一半。因此，**在测试时，直接用dropout 的概率乘以这个输出**，期望值就相同了。

![image-20220414153628593](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141535381.png)

### 总结

总结起来，dropout在正向传播中非常简单，只需要添加两行代码，随机对一些节点置零。在测试的预测函数中，仅添加一点乘法，用输出乘以概率即可。Dropout 对于正则化神经网络有很大帮助。

有一个小的tips，如果在测试时你很关心效率，想要测试更快，可以消除乘以p的这一做法，可以在训练时用整个的权值矩阵除以p。因为训练时发生在GPU上的，一个额外的乘法不算什么。

![image-20220414155033227](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141550323.png)

<u>**Q：dropout对梯度有什么影响？**</u>

A：我们只在未被丢弃的节点上传递梯度。当使用了dropout时，训练通常需要更长的时间。因为在每一步，你只是更新网络的一部分。使用了dropout之后，通常需要更多的时间去训练，但是收敛后，模型的鲁棒性更好。

## 通用的正则化策略

有一个通用的策略用来做正则化，**在训练期间我们给网络添加一些随机性，一定程度上扰乱网络，防止过拟合。测试的时候我们要抵消掉所有随机性，提高泛化能力。**dropout 就是这种通用策略的一个具体实例。

![image-20220414161345988](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141613041.png)

batch normalization 也具有这种思想。用batch normalization 做训练时，一个数据点和其他不同的数据点可能出现不同的小批量中。对于单个数据点来说，在训练过程中进行正则化具有随机性，但是在测试过程中我们通过使用基于全局估计的正则化来抵消掉这个随机性。实际上，batch normalization 倾向于具有和dropout类似的正则化效果，都具有这种通用的正则化策略的思想。

实际上，在使用batch normalization 训练神经网络时，可能不使用dropout，因为仅仅是batch normalization 就给了网络足够的正则化效果。不同的是，dropout可以通过改变概率 $p$来调整正则化的力度，batch normalization 没有这种控制机制。

## 数据增强

另外一种符合通用正则化策略的是 **数据增强(data augmentation )**。训练最初，有原始的数据和各自的标签。在训练的过程中，以某种方式随机地转换图像，保留标签不变，然后使用转换后的图像来进行训练。

在训练时可以采取水平翻转，裁剪不同尺寸的方式增加随机性。训练时通过评估一些固定的裁剪图像来抵消随机性，通常是四个角落，中间以及它们的翻转。比较常见的就是在ImageNet的论文时，作者会总结他们模型的单个裁剪图像效果和他们模型的10种裁剪式的效果——包括5种标准裁剪加上它们的翻转。

在数据增强中有时会使用色彩抖动——随机改变图像的对比度和亮度。可以使用色彩抖动来等到一些更复杂的图像。当你试图在数据空间的主成分分析方向上产生色彩抖动，你会以某种与数据相关的方式进行色彩抖动。这不太常见。

一般来说数据增强是非普遍的事情，你可以将应用于任何问题。**在不更改标签的前提下对数据进行转换，在训练的时候将转换后的数据应用于你的输入数据。**这种方式对网络有正则化效果，因为在训练的时候你又增加了某种随机性，然后在测试的时候将它们淡化。

## Drop connect

还有一种与*dropout 相关的算法叫做drop connect，但不是在每次正向传播中将激活函数置零，而是随机将权重矩阵的一些值置零。它们的效果是相同的。

![image-20220414165904578](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141659628.png)

## 分数阶最大池化 Fractional Max Pooling

一般进行最大池化时，会固定2*2的区域，在前向传播的前面进行池化。分数阶最大池化就是每次在池化层时，进行随机池化（不采用固定大小，随机选取区域）。
下图中展示了3个不同的在训练时可能遇到的随机池化区域。
测试时，有很多方法可以抵消随机性。比如使用一些固定的池化区域，或者选取很多样本对它们取平均。这种想法很好，但不常用。

![image-20220414170620379](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141706463.png)

## 随机深度 Stochastic Depth

如图所示，我们有一个很深的网络。**在训练时，我们随机的从网络中丢弃部分层，在测试的时候我们用全部层**。
在实际操作的时候并不常用，但它的想法不错。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141710532.png" alt="image-20220414171038489" style="zoom: 50%;" />

**<u>Q：你经常使用超过1 个的正则化方法么</u>**

A：通常使用 batch normalization，它是一个现在大多数网络使用的方法。它可以帮助收敛特别是非常深的网络。大多数情况下单独使用 batch normalization 就够了。如果batch normalization 单独使用不太够，你可以增加dropout 或一些其他的东西。一般不要盲目地交叉验证这些方法，而是有的放矢。在网络过拟合时把它们加进去。