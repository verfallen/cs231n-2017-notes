# 正则化

如何提高单一模型的效果呢？**使用正则化**。正则化的思想是**在模型中加入一些成分来防止训练集上的过拟合，从而使测试集上的效果得到提升**。在前面已经经过几种正则化的方法，如下图所示，我们在损失函数上加入额外的一项（红框圈住的部分）。前面的一项是用来拟合数据，后面的一项是用来防止模型过拟合的，称为**正则项**。

![image-20220414134530394](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141345442.png)

常用的正则化方法如下：

![image-20220414134745917](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141347971.png)

## Dropout 

在神经网络中，L2 正则化在神经网络中效果不明确，因此我们选择其他方案。一个在神经网络中常用的方法就是 **Dropout**。Dropout 的思想非常简单，**每次在网络中正向传递时，我们在每一层网网络中，随机地将一部分神经元置为 0** 。每次进行正向传递时，随机被置零的神经元都不是完全相同的，在处理单层网络事，算出这一层的值然后随机将其中一些置为0，接着继续在网络中前进。

对比下面的左右图，左图是全连接网络，右图是经过Dropout 的版本，可以发现，**经过 dropout 之后的网络就是同样的网络变小了一号，只用到其中一部分神经元**。

![image-20220414135413015](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141354068.png)

<u>**Q：我们把什么置为0？**</u>

A：激活函数的输出值。每一层都是在计算上一层的激活函数的结果乘以权值矩阵，得到下一个激活函数前的结果。然后我们计算激活函数，将其中一部分置为0，那么下一层拿到的激活函数的输出里面就有一部分是0。

<u>**Q：在哪些层里使用Dropout？**</u>

A：一般在全连接层或者卷积层。但是在卷积层中使用 dropout，有时候并不是随机将某个神经元上的激活函数的输出置为0，而是随机把某个特征映射置为0。在CNN中，有一个维度表示通道，有可能要把某几条通道整个置为0,而不是将元素置0。

### 代码实现

dropout 的代码实现非常简单，只需要两行。下图是一个三层的神经网络中实现 dropout 的实例。

![image-20220414140256626](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141402706.png)

### dropout 为什么有用

一个比较勉强的解释是，人们认为dropout 避免了特征间的相互适应。举例来说要在分类中判断是不是猫，可能在网络中，一个神经元学到了耳朵，一个神经元学到了尾巴，另一个学到了输入图像有毛，然后将这些特征组合起来判断。加入dropout  后，网络不是依赖将这些特征组合在一起给出结果，而是通过不同的零散特征来判断。这也许在某种程度上抑制了过拟合。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141412424.png" alt="image-20220414141212377" style="zoom:50%;" />

另一个解释是，dropout 是在单一模型中进行的集成学习。观察经过dropout的网络，我们是在一个全连接网络的子网络中进行，也就是用全部神经元的子集进行运算。每一种可能的dropout 方式都可以产生一个不同的子网络。所以dropout 像是同时对一群共享参数的网络来进行集成学习。又因为 dropout 的可能性是随着神经元个数呈指数级增长的，你不可能穷举每种情况。这可以看做是一个超级无极大的网络集合在同时被训练。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141419485.png" alt="image-20220414141937411" style="zoom:50%;" />

### Test Time

当使用了dropout ,我们把神经网络基本的运算都改变了，引入了一个额外$z$，代表在dropout中被随机置零的项。但是在测试时引入随机性可能不是一个好主意，想象一下你在Facebook工作，想要对用户上传的图片进行分类，今天分类器的判断是猫，明天也不是了，这就很糟糕。我们希望一个网络一旦被训练好了，在测试时能够消除这种随机性，我们就会想要将随机性进行平均。如果把它写下来，可以想象通过积分来边缘化随机性。

![image-20220414153358697](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141533738.png)

但在实际中，难以对积分求解。可以通过采样来逼近这积分。对z进行对此采样，然后再测试时将采样结果平均化。这仍然会引入一些随机性。

在dropout 的情况下，可以使用更省事的方式来局部逼近这个积分。假设有这样一个神经元，$a=w_1 \cdot x + w_2 \cdot y$

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141537919.png" alt="image-20220414153710879" style="zoom:33%;" />

在训练时使用了dropout，丢弃神经元的概率是0.5。有4种可能的dropout掩码集合，通过将这4个掩码的集合进行平均，可以求出训练期间的期望值。可以看出，训练时的平均值只有测试的一半。因此，**在测试时，直接用dropout 的概率乘以这个输出**，期望值就相同了。

![image-20220414153628593](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141535381.png)

### 总结

总结起来，dropout在正向传播中非常简单，只需要添加两行代码，随机对一些节点置零。在测试的预测函数中，仅添加一点乘法，用输出乘以概率即可。Dropout 对于正则化神经网络有很大帮助。

有一个小的tips，如果在测试时你很关心效率，想要测试更快，可以消除乘以p的这一做法，可以在训练时用整个的权值矩阵除以p。因为训练时发生在GPU上的，一个额外的乘法不算什么。

![image-20220414155033227](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204141550323.png)

<u>**Q：dropout对梯度有什么影响？**</u>

A：我们只在未被丢弃的节点上传递梯度。当使用了dropout时，训练通常需要更长的时间。因为在每一步，你只是更新网络的一部分。使用了dropout之后，通常需要更多的时间去训练，但是收敛后，模型的鲁棒性更好。