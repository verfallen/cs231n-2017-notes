# 回顾

跟随 Image Net分类挑战比赛中获胜者的脚步，上节课我们讲的是 CNN 架构。

首先是 2012年的 **AlexNet架构**，**它是一个9层的卷积网络，它在计算机视觉领域开展了一场深度学习的革命。**

然后是2014年的**VGG**和**GoogleNet**，它们的构架变得更深。**VGG是拥有16层和19层的模型，Google Net是一个22层的模型。**由于在2014年，批量归一化还没有出现，训练约20 层的深层模型是非常具有挑战性的。所以，这两种模型都需要借助一些技巧才能使它们收敛。对于 VGG，它有16层和19层的模型，首先训练了一个11 层的模型，让这个较为浅的模型先收敛，然后在中间添加一些额外的随机层继续训练，便得到了16层和19层的模型。对于GoogleNet，使用了一些辅助分类器，这不是为了让获得更好的分类性能，是一种可以将额外的梯度直接注入到网络尾部中的方法。有了批量标准化，这些技巧都不再需要了。

接着是2015年的**ResNet**，称为**残差网络，它通过一些小的残差块连接。我们将输入传递到残差块，然后加上卷积层的输出。这是一种有趣的构架**，它有两个属性，一是如果我们把残差块中所有的权值设为零，那么所有的残差块就是恒等的。在某种程度上，这个模型相对容易训练。而且这解释了在神经网络中使用L2正则化的原因。L2正则化会迫使参数趋近于0。在残差网络中参数逐渐趋向于0，那就是促使模型不再使用它不需要的层，驱使残差块走向恒等。另一个属性是残差网络与反向路径中的梯度流有关，如果你还记得在反向传播中加法门的工作原理，当上游梯度通过加法门时，将沿着两个不同的路径，一是通过卷积块，二是通过残差连接直接连接到梯度。当成百上千个残差块堆叠在一起时，残差连接给梯度提供了一条“高速公路”，使梯度在整个网络中反向传播，这让网络的训练变得更快更容易。

在机器学习领域中，管理模型梯度流是非常重要的思想。在递归神经网络中也是很普遍的。

然后是**DenseNet**,**FractalNet** 。**这类模型中都加入了一些额外的恒等连接或梯度的快捷方式**。这些额外的拓扑结构让梯度从网络末端损失层流向整个网络中的不同层，提供了一个直接的路径。在 CNN 构架中合理地管理梯度流，是我们在过去几年里看到的最多的东西。

然后我们看到不同模型中浮点运算量，参数数量和运行时间的比较。可以发现，**像 VGG和 AlexNet 这样的大型模型，拥有大量的参数，大部分来自模型中的全连接层。**如AlexNet大概有6200万个参数，它最后面的全连接层，激活尺寸从6×6×256变成4096的全连接向量。这个权值矩阵的元素个数是6x6x256x4096，约3800万个参数。因此在AlexNet 模型中，一半以上的参数，只存在于最后的全连接层中。对比其他架构，如 GoogleNet 和ResNet ，这些神经网络没有使用这些大型的全连接层，**而是在神经网络的末端使用全局平均池化。**这使得神经网络有更好的架构。大幅降低参数的数量。这就是关于 CNN 架构的简要回顾。

# 任务分类

机器学习中会有各种任务的分类，根据输入和输出是否可变，可以分为以下类别。

## 一对一

这是我们之前最常看到的基础网络结构，**输入接受固定尺寸的对象**，比如一幅图片或者一个向量，通过隐层给出**单一的输出结果**，比如一个值，一个分类或者一组类别，叫做**香草（vanilla）前馈网络**。它的结构是一对一的。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204261248455.png" alt="image-20220426124802065" style="zoom:50%;" />

## 一对多

一对多的网络结构，**输入是固定尺寸的对象**，**输出是长度可变的序列**。比如图片描述就是这样一个任务。不同的描述可能造成单词量的不同，因此输出值的长度需要是一个变量。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204261249913.png" alt="image-20220426124938883" style="zoom:50%;" />

## 多对一

输入的尺寸是可变的，输出是固定的。比如输入一段文字，判断文字的情感属性是消极情感还是积极情感。或者在输入一个视频，整个视频的时间是不固定的，做出分类决策，判断视频中做了什么活动。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204261254270.png" alt="image-20220426125411229" style="zoom:50%;" />

## 多对多

输入和输出都是可变的，比如机器翻译，输入英文句子，输出法文句子，英语句子的长度可能与法语句子不同，因此需要模型能够同时容纳输入和输出的长度可变序列。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204261256406.png" alt="image-20220426125606357" style="zoom:50%;" />

另外还有一种，也是多对多，但是输入和输出的长度是相同的。例如一段有序的视频，帧数是一个变量，我们要对该序列中的每个元素做出决策，在输入是视频的情况下，对每一帧都做分类决策。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204261423504.png" alt="image-20220426142336472" style="zoom:50%;" />

# RNN

RNN是**Recurrent Neural Networks，递归神经网络**的简称。

它是用于**处理长度可变的有序数据的一类模型**，让我们可以比较自然地理解模型的不同的架构。**即使对有固定输入大小**
**和固定输出大小的问题，RNN 也同样很有用。**

举个例子，我们收到了一个固定大小的输入，如一幅图像，要做出分类决策，判断图像中的数字是多少。不是做单一的前向传播，马上做出决策，而是观察图片的各种不同部分之后，做出最终决策，判断数字到底是几。

在这一例子中，输入是图片，输出是分类决策。在这样的情况下
利用 RNN 网络来处理长度可变的序列数据，也会形成一些有意思的模型。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204271213135.png" alt="image-20220427121001478" style="zoom:50%;" />



有一份论文[ A Recurrent Neural Network For Image Generation”](http://proceedings.mlr.press/v37/gregor15.html)运用了这一想法来生成新的图像。任务是模型来合成
新的图片，这些图片看上去像之前训练的那些图片相似。
可以用 RNN 架构来绘制出这些输出图像，大约一次绘制一幅图像。虽然输出是固定尺寸的图像，我们可以让这些模型一直工作
每次计算出一部分输出，这样就可以用 RNN来完成了。

![image-20220427121950821](https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204271219098.png)

## 原始循环神经网络

那么RNN到底做的是什么呢？

每个 RNN 网络都有这样一个小小的**循环核心单元**，它把 x 作为输入，RNN 有一个**内部隐藏态(internal hidden state )**，这一隐藏态会在 RNN 每次读取新的输入时更新。当模型下一次读取输入时，内部隐藏态会反馈至模型 。因此，就有了这样的模式 ，**读取输入，更新隐藏态，并且生成输出。**

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204271300899.png" alt="image-20220427130059858" style="zoom:33%;" />

用公式来表达的话，就是 
$$
h_{t} = f_W(h_{t-1},x_t)
$$
其中，$h_{t}$ 表示更新的隐藏态，$h_{t-1}$ 表示上一次的隐

藏态，$x_t$ 表示当前输入，$f_W$ 表示依赖权重 $W$ 的函数。

在RNN的模块中，我们使用函数 f 对循环关系进行计算。函数接收隐藏态$h_{t-1}$ 和 $x_t$ ，输出更新后的隐藏态 $h_t$。读取下一个输入时，$h_t$ 和 $x_{t+1}$ 参数再次传入函数 $f_W$ 。 如果想要在网络的每一步都生成输出，可以增加全连接层，每一步都将$h_t$ 作为输入，根据每一步的隐藏态做出决策。要注意的一点就是，用的是同样的函数$f_W$，同样的权重$W$。

使用 $tanh$ 作为  f 函数，可以写成下式。用权值矩阵 $W_{hh}$和输入 $x_t$ 相乘，另一个权值矩阵 $W_{hh}$ 和 $h_{t-1}$ 相乘，再将两个相乘的结果详见，最后用tanh 函数将结果缩放到-1~1 中。这样，系统中就引入了一些非线性元素。
$$
h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t) \\
y_t = W_{hy}h_t
$$

## 计算图

### 多对多计算图

当我想到-RNN 时，从两方面来思考它。第一，它有一种隐藏态
可以循环反馈给自我。这张图有让人疑惑，将这张计算图展开更多的时步，隐藏态里的数据传输流，以及输入，输出，权重的走向就变得更为清晰了。

在第一时步，有初始隐层状态$h_0$，通常情况下$h_0=0$,有输入项 $x_t$，会被代入$f_W$ 函数中，计算得出下一个隐层状态$h_1$。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204271434429.png" alt="image-20220427143433393" style="zoom:33%;" />

得到下一个输入项后，重复这个过程。将$h_1$和 $x_2$代入之前的方程$f_W$，得到新的输出项 $h_2$。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204271437144.png" alt="image-20220427143756107" style="zoom: 33%;" />

这个过程将会不断重复，直到用完输入序列的输入项$x_t$。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204271439242.png" alt="image-20220427143908187" style="zoom: 33%;" />

现在我们可以让这个过程更为清楚一些，将权重矩阵写在我们的计算流程图上。能看到在每个步长中，**使用相同的权重矩阵W**。这个$f_W$ 块，虽然每次接收不同隐藏态和不同的x，但使用相同的$W$ 权重。回想一下反向传播中的梯度流过程，在一张计算图中多次重复使用的相同节点，就在回溯过程中，不断地计算 $d loss/dw$，并最终把所有梯度值加到w 矩阵上。
如果将反向传播的原理应用到这个模型中，你会得到在每一个时步下计算出的梯度。最终的w 梯度是所有时步下独立计算出的梯度之和。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204271447278.png" alt="image-20220427144748230" style="zoom:33%;" />

同样可以直接把 $y_t$写在这张计算图上，这样每个计算步长下输出的$h_t$，作为输入给之后的神经网络，输出该时步下的$y_t$。
$y_t$ 可以是每个时步的类别得分或是其他类似的东西。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204271447190.png" alt="image-20220427144702128" style="zoom: 33%;" />

然后来看损失。在大多数情形下，每一个时步都有一个与输入序列对应的真实标签。这样就可以计算出每个时步下输出相对应的损失值，通常是`softmax` 损失之类。计算这样的损失，需要序列在每个时步下都有与之对应的真实标签。最终的损失值是这整个训练中这些单独的损失值的总和。得到每个时步的损失值，把它们加起来，就得到了最终的损失值。

<img src="https://raw.githubusercontent.com/verfallen/cs231n-2017-notes/main/img/202204271451110.png" alt="image-20220427145107030" style="zoom:33%;" />

为了训练这个模型，我们需要计算损失函数在 W 上的梯度。最终的损失值又会回溯到每一个时步的损失，然后每一个时步又会各自计算出在权重 $w$ 上的梯度。它们的总和就是权重$w$的最终梯度。