# 回顾

跟随 Image Net分类挑战比赛中获胜者的脚步，上节课我们讲的是 CNN 架构。

首先是 2012年的 **AlexNet架构**，**它是一个9层的卷积网络，它在计算机视觉领域开展了一场深度学习的革命。**

然后是2014年的**VGG**和**GoogleNet**，它们的构架变得更深。**VGG是拥有16层和19层的模型，Google Net是一个22层的模型。**由于在2014年，批量归一化还没有出现，训练约20 层的深层模型是非常具有挑战性的。所以，这两种模型都需要借助一些技巧才能使它们收敛。对于 VGG，它有16层和19层的模型，首先训练了一个11 层的模型，让这个较为浅的模型先收敛，然后在中间添加一些额外的随机层继续训练，便得到了16层和19层的模型。对于GoogleNet，使用了一些辅助分类器，这不是为了让获得更好的分类性能，是一种可以将额外的梯度直接注入到网络尾部中的方法。有了批量标准化，这些技巧都不再需要了。

接着是2015年的**ResNet**，称为**残差网络，它通过一些小的残差块连接。我们将输入传递到残差块，然后加上卷积层的输出。这是一种有趣的构架**，它有两个属性，一是如果我们把残差块中所有的权值设为零，那么所有的残差块就是恒等的。在某种程度上，这个模型相对容易训练。而且这解释了在神经网络中使用L2正则化的原因。L2正则化会迫使参数趋近于0。在残差网络中参数逐渐趋向于0，那就是促使模型不再使用它不需要的层，驱使残差块走向恒等。另一个属性是残差网络与反向路径中的梯度流有关，如果你还记得在反向传播中加法门的工作原理，当上游梯度通过加法门时，将沿着两个不同的路径，一是通过卷积块，二是通过残差连接直接连接到梯度。当成百上千个残差块堆叠在一起时，残差连接给梯度提供了一条“高速公路”，使梯度在整个网络中反向传播，这让网络的训练变得更快更容易。

在机器学习领域中，管理模型梯度流是非常重要的思想。在递归神经网络中也是很普遍的。

然后是**DenseNet**,**FractalNet** 。**这类模型中都加入了一些额外的恒等连接或梯度的快捷方式**。这些额外的拓扑结构让梯度从网络末端损失层流向整个网络中的不同层，提供了一个直接的路径。在 CNN 构架中合理地管理梯度流，是我们在过去几年里看到的最多的东西。

然后我们看到不同模型中浮点运算量，参数数量和运行时间的比较。可以发现，**像 VGG和 AlexNet 这样的大型模型，拥有大量的参数，大部分来自模型中的全连接层。**如AlexNet大概有6200万个参数，它最后面的全连接层，激活尺寸从6×6×256变成4096的全连接向量。这个权值矩阵的元素个数是6x6x256x4096，约3800万个参数。因此在AlexNet 模型中，一半以上的参数，只存在于最后的全连接层中。对比其他架构，如 GoogleNet 和ResNet ，这些神经网络没有使用这些大型的全连接层，**而是在神经网络的末端使用全局平均池化。**这使得神经网络有更好的架构。大幅降低参数的数量。这就是关于 CNN 架构的简要回顾。
